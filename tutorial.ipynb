{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we need to manage the energy flow of a building powered by solar panel system with a battery storage as shown in Figure 1. When solar PV generation is not sufficient to cater to the demand, the main grid supplies the mismatch at a varying (with time) price. Further the cost of $CO_2$ produced at a specific time by the grid is also considered for the cost.\n",
    "\n",
    "<img src=\"files/data/building.png\" width=550>\n",
    "<p style=\"text-align: center; font-weight:bold;\">Figure 1: Set up of building energy management system</p>\n",
    "\n",
    "Our task in this tutorail is to charge or discharge the battery accordingly so that the overall cost is minimized.\n",
    "\n",
    "This tutorial is based on the simulation engine [Pymgrid](https://github.com/Total-RD/pymgrid) by Total-RD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will download the (custom) pymgrid library\n",
    "!pip install git+https://github.com/Wenuka/pymgrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pymgrid import MicrogridGenerator as mg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Microgrids (a.k.a Building+PV+Battery+grid System)\n",
    "\n",
    "In this specific case, the microgrid is defined as the building (energy demand) Solar panel (PV), inhouse battery pack which is also connected to the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we create the environment and the microgrid\n",
    "env = mg.MicrogridGenerator(nb_microgrid=1, path=\"./data/pymgrid_data\")  # we can use a custom file to simulate the load and the PV\n",
    "env.generate_microgrid(verbose=False)\n",
    "mg0 = env.microgrids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PV': 1, 'battery': 1, 'genset': 0, 'grid': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg0.architecture  # architecture of the microgrid \n",
    "# mg0.parameters  # parameters of the microgrid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In this tutorial, we do not consider a generator (genset). Possible ways to integrate generation will be taken into the discussion at the end of the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining the Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL always works with reward. Therefore, we need to define the reward at the begining. Then we can write algorithms so that our agent can learn to maximize the reward.\n",
    "\n",
    "In this, we are trying to reduce (Minimize) the overall cost of the building. Therefore we can easily define the reward as the **minus cost**. So agent maximizing the reward means minimizing the cost.\n",
    "\n",
    "The total cost consists of the cost of eleectricity and the penalty for $CO_2$ produced by the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of the system explains the present conditions of the system. \n",
    " \n",
    "As we already learned, in Q learning, we try to guess the expected discounted future reward for each **State** for each **action**. \n",
    "\n",
    "Therefore, we maintain a Q-table to store these values. By doing this, we maintain rows for each State and columns for each Action (see Figure 2). So each value inside the Q-table is the Q-value (expected discounted future reward) for a given state and for a given action\n",
    "\n",
    "<img src=\"files/data/qtable.png\" width=550>\n",
    "<p style=\"text-align: center; font-weight:bold;\">Figure 2: Q table consisting of actions and states</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Defining the State "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the learning process, we need to define the states and the actions. \n",
    "\n",
    "Let us define the states first. To help the process, we will first plot a few graphs for the next 24 time steps (hours).\n",
    "\n",
    "(extra note: forecast function forecast next 24 steps by default you can change it to N steps by **mg0.set_horizon(N)** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7ea2136d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mg0.forecast_pv(), label=\"PV\")\n",
    "plt.plot(mg0.forecast_load(), label=\"load\")\n",
    "plt.legend()\n",
    "# as we can see PV and Load is varying with the  time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ea0027280>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mg0.forecast_grid_status())  \n",
    "# Here we observe that the microgrid is always connected to the utility grid (no change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the environment in this is defined by four factors (Figure 3), namely,\n",
    "* Load (kW)\n",
    "* PV (kW)\n",
    "* Battery (charge percentage)\n",
    "* Grid connectivity (Grid price is not provided by the simulation environment)\n",
    "\n",
    "<img src=\"files/data/environment.png\" width=500>\n",
    "<p style=\"text-align: center; font-weight:bold;\">Figure 3: Energy interactions to state definition</p>\n",
    "\n",
    "So, we can simply describe state by **(load, PV, Battery Percentage, Grid connectivity)** where Battery Percentage is (battery charge level \\state of charge of battery)\n",
    "\n",
    "1. Do we need all these variables to define the state? \n",
    "\n",
    "2. What variables/factors can you eliminate? \n",
    "\n",
    "3. Given our problem statement, what variables can you combine?\n",
    "\n",
    "**Discuss!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--answer--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem we face here is how to create Q-table when the variables in the state definition are continuous. \n",
    "\n",
    "For this, we **discretize the variable**. For example in this tutorial, we round the kW to the nearest decimal and battery percentage to the nearest 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg0.set_horizon(24*365)\n",
    "\n",
    "overall_first_state_variable = mg0.forecast_load()-mg0.forecast_pv()  # ???\n",
    "\n",
    "overal_min = min(overall_first_state_variable)\n",
    "overal_max = max(overall_first_state_variable)\n",
    "discretize_interval = 1\n",
    "net_load_params = (overal_min, overal_max, discretize_interval)\n",
    "mg0.set_horizon(24)\n",
    "f\"max: {overal_max:.2f}, min : {overal_min:.2f} interval: {discretize_interval}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given instences, you can see the environment and other values like this\n",
    "current_load = mg0.load\n",
    "current_pv = mg0.pv\n",
    "battery_capacity = mg0.battery.capacity\n",
    "battery_state_of_charge = mg0.battery.soc\n",
    "# current_cost = mg0.battery.get_cost() ## can be used only after running atleast one step of the microgrid (eg. mg.run(action) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Designing the Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have defined the states, let's try to define the actions that our agent need to perform to maximize the reward. \n",
    "\n",
    "These are the four actions that we are suggesting\n",
    "* action 0: battery_charge\n",
    "* action 1: battery_discharge\n",
    "* action 2: grid_import\n",
    "* action 3: grid_export\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our simulation engine does not know what these actions are.\n",
    "\n",
    "For this, we have to map these actions to the simulation engines control signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions_agent(mg0, action):\n",
    "    pv = mg0.pv\n",
    "    load = mg0.load\n",
    "    net_load = load - pv\n",
    "    capa_to_charge = mg0.battery.capa_to_charge  # remaining capacity to charge of the battery\n",
    "    p_charge_max = mg0.battery.p_charge_max  # charge speed of the battery\n",
    "    p_charge = max(0,min(-net_load, capa_to_charge, p_charge_max))  # charge value for the time \n",
    "    \n",
    "    capa_to_discharge = mg0.battery.capa_to_discharge  # capacity of discharge\n",
    "    p_discharge_max = mg0.battery.p_discharge_max  # per hour discharge rate\n",
    "    p_discharge = max(0,min(net_load, capa_to_discharge, p_discharge_max))  # discharge value for the time \n",
    "          \n",
    "    if action == 0:       \n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': p_charge,\n",
    "                        'battery_discharge': 0,\n",
    "                        'grid_import': 0,\n",
    "                        'grid_export':max(0,pv - min(pv,load) - p_charge)\n",
    "                       }            \n",
    "    elif action ==1:    \n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': 0,\n",
    "                        'battery_discharge': p_discharge,\n",
    "                        'grid_import': max(0,load - min(pv,load) - p_discharge),\n",
    "                        'grid_export':0\n",
    "                       }\n",
    "    elif action ==2:\n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': 0,\n",
    "                        'battery_discharge': 0,\n",
    "                        'grid_import': abs(net_load),\n",
    "                        'grid_export':0\n",
    "                       }\n",
    "    elif action == 3:\n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': 0,\n",
    "                        'battery_discharge': 0,\n",
    "                        'grid_import': 0,\n",
    "                        'grid_export':abs(net_load)\n",
    "                       }\n",
    "    \n",
    "    return control_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initializing the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_qtable(mg0, nb_action, net_load_params):\n",
    "\n",
    "    state = []\n",
    "    Q = {}\n",
    "    \n",
    "    # fill in here the range you need to add for the first param of the state\n",
    "    # last 1 in the range is the discretize interval\n",
    "    for i in range(???, ???, 1): \n",
    "        \n",
    "        # fill in here the range you need to add for the second param of the state\n",
    "        # last 0.1 in the range is the discretize interval\n",
    "        for j in np.arange(???, ???, 0.1):\n",
    "            \n",
    "            j = round(j,1)\n",
    "            state.append((i,j)) # then add all the states to the variable \"state\" as tuple (1st param, 2nd param)\n",
    "    \n",
    "    # now that we have all the possible states, let's create the Q table\n",
    "    for s in state:\n",
    "\n",
    "        Q[s] = {}  # for each state we store a dictionary (a row in Q-table)\n",
    "\n",
    "        for a in range(???):\n",
    "            # add initial value for Q table, for each s and a\n",
    "            ???\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy** is the way we select the action, given the Q-values. \n",
    "\n",
    "In this tutorial we have selected the famous **epsilon greedy policy**.\n",
    "\n",
    "To explain what this is, given a state, we act greedily (choose the action with the highest Q-value) 1-epsilon times while we randomly select an action epsilon of the time where the epsilon is a positive decreasing value less than 1. If interested, please take a look into the helper_functions.py where we have defined the policy.\n",
    "\n",
    "Since we act randomly epsilon times, our agent gets the chance to explore the environment. Since we decrease the epsilon with time, the agent takes less and less random actions and always tries to act greedily.\n",
    "\n",
    "Selecting epsilon is an important task and this is normally called the *Explore-Exploit dilemma.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is the Q-learning algo (from Sutton and Barto's book)\n",
    "<img src=\"files/data/QlearningAlgo.png\">\n",
    "\n",
    "Now let's implement it for our application,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Q_Learning(mg, horizon, net_load_params, nb_episode = 100):\n",
    "    \n",
    "    nb_action = 4\n",
    "    Q = ???  # initialise Q-table\n",
    "    nb_state = len(Q)\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.99\n",
    "    gamma = 0.99\n",
    "    \n",
    "    record_cost = []\n",
    "    t0 = time.time()\n",
    "    t = t0\n",
    "    print_training = \"Training Progressing\"\n",
    "    print_welcome(0)\n",
    "    for e in range(nb_episode+1):\n",
    "\n",
    "        # to print\n",
    "        value_print=\"\\r\"+ print_training +\"Episode \" + str(e) +\"/\" + str(nb_episode) \n",
    "        sys.stdout.write(value_print)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # learning start\n",
    "        episode_cost = 0\n",
    "        mg.reset() # reset the environment\n",
    "        \n",
    "        net_load = ??? # mg.load and mg.pv can be used to get load and pv at a given time\n",
    "        soc = ??? # mg.battery.soc to get the charge of the battery\n",
    "        s = (???, ???)\n",
    "        \n",
    "        for i in range (horizon):\n",
    "            \n",
    "            a = ??? # action with max value for the given state (refer helper_function.py for a hint)\n",
    "            a, is_random = ???  # action after epsilon greedy policy (refer helper_function.py for a hint)\n",
    "            action = actions_agent(mg,a)  # returns control_dict for the action\n",
    "            \n",
    "            status = mg.run(action) # apply the action in the environment\n",
    "            r = ??? # mg.get_cost() can be used to get the cost at a given time\n",
    "            episode_cost += mg.get_cost()\n",
    "            net_load = ???\n",
    "            soc = ???\n",
    "            s_ = (???, ???)\n",
    "            \n",
    "            Q[s][a] = ???\n",
    "            \n",
    "            s = ???\n",
    "       \n",
    "        epsilon = update_epsilon(epsilon)  # decrease epsilon\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_Q_Learning(mg, Q,horizon):\n",
    "    \n",
    "    mg.reset(testing=True)\n",
    "    net_load = ???\n",
    "    soc = ???\n",
    "    s = (???, ???)\n",
    "    a = ???\n",
    "    total_cost = 0\n",
    "    print_welcome(1)\n",
    "    for i in range (horizon):\n",
    "\n",
    "        action_name = change_name_action(a)\n",
    "        action = actions_agent(mg, a)\n",
    "        status = mg.run(action)\n",
    "        cost = ???\n",
    "        total_cost += cost\n",
    "        print(i,\"-\",(???,???),action_name, round(total_cost,1), \"€\")  # add two variable of the state here\n",
    "        net_load = ???\n",
    "        soc = ???\n",
    "        s_ = (???, ???)\n",
    "        a_ = ???\n",
    "\n",
    "        s, a = s_, a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg0.train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = training_Q_Learning(mg0,48,net_load_params,nb_episode=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Q_Learning(mg0,Q1, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SARSA Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is the SARSA algo (from Sutton and Barto's book)\n",
    "<img src=\"files/data/SARSAalgo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is SARSA called an on-policy learning and Q-learning is an off-policy learning?\n",
    "\n",
    "2. What changes to do in the Q-learning to make it SARSA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--answer here--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. What could go wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. According to the above implementation of states, what could go wrong?\n",
    "2. Any possible improvements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--answer here--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to use Neural networks to achieve a better result of the above Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data/deepQ.png\">\n",
    "<p style=\"text-align: center; font-weight:bold;\">Figure 4: A basic Deep Q-Learning Architecture</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we input the state variables and try to predict the Q values for each state. Since this is a supervised task, what can we use as the target since we do not know the Q-values.\n",
    "\n",
    "So in here, the trick is create a copy of the network and keep one as a target and train the other. To converge the Q values faster, we only update the learnt parameters to the target network only in every **K**th iteration\n",
    "<img src=\"files/data/deepQtrain.png\" width=400>\n",
    "<p style=\"text-align: center; font-weight:bold;\">Figure 5: Training process of the Deep Q-Learning Architecture</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some libraries that have already implemented such networks. One good example if you are using Keras is [The Keras-RL Library](https://github.com/keras-rl/keras-rl). Here we are only supposed to input our deep network with our open-gym like environment.\n",
    "\n",
    "Due to the time constraints, we do not wish to explain this in detail.\n",
    "\n",
    "Please refer to this [example](https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_cartpole.py) if you are interested. \n",
    "\n",
    "[This](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch) might be a good starting point if you are using Pytorch for model implementation.\n",
    "\n",
    "If you are using such an external library, please make sure to use the exact dependency (example same TF and Keras version) for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, It is important to note that Deep Q is really important when the action space is also continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "A. T. D. Perera, R. A. Attalage and K. K. C. K. Perera “An optimal design of a grid connected hybrid electrical energy system using evolutionary computation” IEEE-ICIIS 2013\n",
    "\n",
    "G Henri,T Levent, A Halev, R Alami and P Cordier “pymgrid: An Open-Source Python Microgrid Simulator for Applied Artificial Intelligence Research” https://arxiv.org/abs/2011.08004\n",
    "\n",
    "Richard S. Sutton and Andrew G. Barto “Reinforcement Learning: An Introduction” 2nd Edition The MIT Press, 2017\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
